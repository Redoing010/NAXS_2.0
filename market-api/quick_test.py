#!/usr/bin/env python3
"""
å¿«é€Ÿæ€§èƒ½æµ‹è¯•è„šæœ¬
"""

import asyncio
import aiohttp
import time
import statistics
from concurrent.futures import ThreadPoolExecutor
import json

class QuickPerformanceTest:
    def __init__(self, base_url="http://localhost:8001"):
        self.base_url = base_url
        self.results = []
    
    async def single_request(self, session, endpoint):
        """å‘é€å•ä¸ªè¯·æ±‚"""
        start_time = time.time()
        try:
            async with session.get(f"{self.base_url}{endpoint}") as response:
                await response.text()
                return time.time() - start_time, response.status
        except Exception as e:
            return time.time() - start_time, 0
    
    async def concurrent_test(self, endpoint, num_requests=100, concurrency=10):
        """å¹¶å‘æµ‹è¯•"""
        print(f"ğŸ”¥ Testing {endpoint} with {num_requests} requests, {concurrency} concurrent")
        
        connector = aiohttp.TCPConnector(limit=concurrency)
        async with aiohttp.ClientSession(connector=connector) as session:
            # åˆ›å»ºä»»åŠ¡
            tasks = []
            for _ in range(num_requests):
                task = self.single_request(session, endpoint)
                tasks.append(task)
            
            # æ‰§è¡Œä»»åŠ¡
            start_time = time.time()
            results = await asyncio.gather(*tasks)
            total_time = time.time() - start_time
            
            # åˆ†æç»“æœ
            response_times = [r[0] for r in results]
            status_codes = [r[1] for r in results]
            
            successful_requests = len([s for s in status_codes if s == 200])
            
            stats = {
                'endpoint': endpoint,
                'total_requests': num_requests,
                'successful_requests': successful_requests,
                'success_rate': successful_requests / num_requests * 100,
                'total_time': total_time,
                'requests_per_second': num_requests / total_time,
                'avg_response_time': statistics.mean(response_times) * 1000,  # ms
                'min_response_time': min(response_times) * 1000,
                'max_response_time': max(response_times) * 1000,
                'p95_response_time': self.percentile(response_times, 95) * 1000,
                'p99_response_time': self.percentile(response_times, 99) * 1000
            }
            
            return stats
    
    def percentile(self, data, percentile):
        """è®¡ç®—ç™¾åˆ†ä½æ•°"""
        sorted_data = sorted(data)
        index = int(len(sorted_data) * percentile / 100)
        return sorted_data[min(index, len(sorted_data) - 1)]
    
    async def run_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("ğŸš€ Starting NAXS API Performance Tests...\n")
        
        # æµ‹è¯•ç«¯ç‚¹
        endpoints = [
            '/health',
            '/system/stats',
            '/performance/test'
        ]
        
        all_results = []
        
        for endpoint in endpoints:
            # è½»é‡æµ‹è¯•
            result = await self.concurrent_test(endpoint, num_requests=50, concurrency=5)
            all_results.append(result)
            
            print(f"ğŸ“Š Results for {endpoint}:")
            print(f"  âœ… Success Rate: {result['success_rate']:.1f}%")
            print(f"  âš¡ Requests/sec: {result['requests_per_second']:.1f}")
            print(f"  ğŸ“ˆ Avg Response: {result['avg_response_time']:.2f}ms")
            print(f"  ğŸ“Š P95 Response: {result['p95_response_time']:.2f}ms")
            print(f"  ğŸ“Š P99 Response: {result['p99_response_time']:.2f}ms")
            print()
        
        # é«˜å¹¶å‘æµ‹è¯•
        print("ğŸ”¥ High Concurrency Test...")
        high_load_result = await self.concurrent_test('/health', num_requests=200, concurrency=20)
        all_results.append(high_load_result)
        
        print(f"ğŸ“Š High Load Results:")
        print(f"  âœ… Success Rate: {high_load_result['success_rate']:.1f}%")
        print(f"  âš¡ Requests/sec: {high_load_result['requests_per_second']:.1f}")
        print(f"  ğŸ“ˆ Avg Response: {high_load_result['avg_response_time']:.2f}ms")
        print(f"  ğŸ“Š P95 Response: {high_load_result['p95_response_time']:.2f}ms")
        print()
        
        # æ€»ç»“
        print("ğŸ“‹ Performance Summary:")
        avg_rps = statistics.mean([r['requests_per_second'] for r in all_results])
        avg_response = statistics.mean([r['avg_response_time'] for r in all_results])
        overall_success = statistics.mean([r['success_rate'] for r in all_results])
        
        print(f"  ğŸ¯ Overall Success Rate: {overall_success:.1f}%")
        print(f"  âš¡ Average RPS: {avg_rps:.1f}")
        print(f"  ğŸ“ˆ Average Response Time: {avg_response:.2f}ms")
        
        # æ€§èƒ½è¯„çº§
        if avg_response < 50 and avg_rps > 500 and overall_success > 99:
            grade = "ğŸ† EXCELLENT"
        elif avg_response < 100 and avg_rps > 200 and overall_success > 95:
            grade = "ğŸ¥‡ GOOD"
        elif avg_response < 200 and avg_rps > 100 and overall_success > 90:
            grade = "ğŸ¥ˆ FAIR"
        else:
            grade = "ğŸ¥‰ NEEDS IMPROVEMENT"
        
        print(f"  ğŸ–ï¸ Performance Grade: {grade}")
        
        return all_results

async def main():
    tester = QuickPerformanceTest()
    results = await tester.run_tests()
    
    # ä¿å­˜ç»“æœ
    with open('performance_results.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    print("\nğŸ’¾ Results saved to performance_results.json")
    print("\nğŸ‰ Performance testing completed!")

if __name__ == "__main__":
    asyncio.run(main())